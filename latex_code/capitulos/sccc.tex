% !TeX root = ../libro.tex
% !TeX encoding = utf8

%\setchapterpreamble[c][0.75\linewidth]{%
%	\sffamily
%  Breve resumen del capítulo. TODO
%	\par\bigskip
%}

\chapter{Códigos Convolucionales Cíclicos}\label{ch:cuarto-capitulo}

En el capítulo anterior hemos estudiado los códigos convolucionales. Al igual que con los códigos de bloque, podríamos pensar en definir una nueva subclase de estos códigos que cumpla una propiedad de ciclicidad. No obstante, como ya se comentó, la teoría matemática de los códigos convolucionales está mucho menos desarrollada que la de los códigos de bloque y esta brecha es aún mayor para la teoría matemática de los códigos cíclicos convolucionales.

En 1976, Piret \cite{Piret}  estableció una base matemática  para la teoría de códigos cíclicos convolucionales (CCC). Uno de sus descubrimientos principales fue que la ciclicidad clásica, tal y como la conocemos para los códigos de bloque, no funciona bien para los códigos convolucionales. Sin embargo, descubrió que trabajando en álgebras no conmutativas sí que existía una sorprendente analogía con los códigos cíclicos de bloque, ya que se podían definir los CCCs como ideales principales de un anillo no conmutativo.

Pocos años después, Roos \cite{Roos} observó que estas álgebras no conmutativas que usó Piret eran un caso particular de las extensiones de Ore, definidas por Ore en 1933 \cite{ore1933}. De esta forma, Roos consideró álgebras más generales para la construcción de los CCCs. Sin embargo, estas ideas no fueron profundamente desarrolladas durante esos años, quizás porque los investigadores no estaban muy familiarizados con los polinomios de Ore y su manipulación. No fue hasta 2004 que Fluesing-Luerssen y Schmale \cite{GL2004} desarrollaron un estudio detallado de (como ellos lo llamaron) los $\sigma$-CCCs, en el que se trabaja con la noción de código convolucional $\C$ como un sumando directo del módulo libre $\F[t]^n$.

En este capítulo, vamos a definir el anillo de polinomios de Ore, los cuales nos permitirán construir unos nuevos tipos de CCCs, llamados códigos convolucionales cíclicos sesgados (SCCC), que a diferencia de los $\sigma$-CCCs, se considera a los códigos convolucionales $\C$ como un subespacio vectorial $k$-dimensional del cuerpo de fracciones $\F(t)^n$. Esto nos permitirá utilizar herramientas desarrolladas para polinomios de Ore que solo están disponibles cuando se trabaja con anillos de división.

Las principales referencias bibliográficas para la redacción de este capítulo han sido \cite{gluesingluerssen2019skewpolynomial} y \cite{jacobson1996} para los anillos de polinomios sesgados, además de \cite{cccheide}, \cite{SCCC} y \cite{gomez2017sugiyama} para los SCCC.

\section{Anillos de polinomios sesgados}
En esta sección se van a introducir los anillos de polinomios sesgados, los cuales son esenciales para definir los códigos convolucionales cíclicos sesgados. Estos polinomios, también conocidos como polinomios de Ore, fueron estudiados por Oystein Ore en \cite{ore1933}. La principal particularidad de estos era que sus coeficientes podían pertenecer a un anillo de división arbitrario, conmutativo o no conmutativo, con la restricción de que el grado del producto de dos polinomios sea igual a la suma de los grados de los factores

\begin{definicion}
    Sea $\mathcal{R}$ un anillo, $\sigma$ un endomorfismo de anillos de $\mathcal{R}$ y $\delta$ una $\sigma$-derivación por la izquierda de $\mathcal{R}$, es decir, $\delta$ es aditivo y para $a,b \in \mathcal{R}$ se verifica que 
    $$ \delta(ab) = \sigma(a)\delta(b) + \delta(a)b.$$

    Entonces el anillo $\mathcal{R}[t;\sigma,\delta]$ de los polinomios en $\mathcal{R}[t]$ de la forma 
    $$a_0 + a_1t + \cdots + a_nt^n,$$
    donde $a_i \in \mathcal{R}$, con la igualdad y suma usuales y donde la multiplicación verifica que $$ta = \sigma(a)t + \delta(a), \ \forall a \in \mathcal{R},$$
    se conoce como el \emph{anillo de polinomios de Ore} o \emph{anillo de polinomios sesgados} y sus elementos reciben el nombre de \emph{polinomios sesgados o de Ore.}
\end{definicion}

Se puede verificar que, efectivamente, $\mathcal{R}[t;\sigma,\delta]$ con las operaciones que hemos definido \emph{es un anillo}. Para ello, debemos verificar que cumple todas las propiedades de anillo descritas en \ref{def:anillo}. Puesto que estamos utilizando la suma usual, solo debemos demostrar que se verifica la propiedad asociativa para la multiplicación. Se va a omitir la demostración, ya que los polinomios de Ore no son el objeto principal de este trabajo, no obstante, esta se puede encontrar en \cite{jacobson1996}.

\subsection{Anillo de polinomios sesgados sobre un cuerpo}\label{sec:ore}

Para el estudio de los códigos cíclicos convolucionales sesgados nos será de utilidad un caso particular de los anillos de polinomios sesgados, donde el anillo $\mathcal{R}$ es un cuerpo $\K$ arbitrario, $\sigma \in \text{Aut}(\K)$ y $\delta = 0$. Denotaremos a estos anillos por $\K[t;\sigma].$

En el anillo $\K[t;\sigma]$ la multiplicación es de la forma $$ta = \sigma(a)t, \ \forall a \in \K.$$

Si $\sigma = Id$, entonces $\K[t;\sigma] = \K[t]$, es decir, es un anillo de polinomios conmutativo sobre un cuerpo $\K$ en el sentido clásico. En general, los grupos aditivos de los anillos $\K[t]$ y $\K[t;\sigma]$ son idénticos, mientras que la multiplicación en $\K[t;\sigma]$ de dos polinomios sesgados $f,g \in \K[t;\sigma]$ viene dada por

\begin{equation}\label{eq:prods} \left(\sum_{i=0}^{n} a_it^i\right)\left(\sum_{j=0}^{m}b_jt^j\right) = \sum_{i,j}a_i\sigma^i(b_j)t^{i+j},\end{equation}


siendo \ $f(t) = a_0 + a_1t + \cdots + a_nt^n$ y $g(t) = b_0 + b_1t + \cdots + b_mt^m.$

Dado un polinomio sesgado $f \in \K[t;\sigma]$, $f \neq 0$, su grado se define de manera usual, es decir, como el mayor exponente de la variable $t$ que aparece en la expresión de $f(t)$ como suma de monomios. Se denota como $\gr(f)$. Además, si $f = 0$, $\gr(f) := - \infty$. El elemento $a \in \K$ que multiplica a la variable de mayor grado recibe el nombre de \emph{coeficiente líder} y se denota como $\text{cl}(f)$.

Esta definición no depende del lado en el que se encuentren los coeficientes, ya que $\sigma$ es un automorfismo. De esta forma, en base a la identidad \eqref{eq:prods} obtenemos que dados dos polinomios sesgados $f,g \neq 0$

$$\gr(fg) = \gr(f) + \gr(g),$$
$$\gr(f + g) \leq \max\{\gr(f),\gr(g)\}.$$

Esto implica que $\K[t;\sigma]$ es un dominio de integridad, ya que dados $f,g \in \K[t;\sigma]$, se tiene que $fg = 0 \Leftrightarrow \gr(fg) = \gr(f) + \gr(g) = -\infty \Leftrightarrow f = 0 \text{ ó } g = 0.$ Por tanto, $\K[t;\sigma]$ no tiene divisores de cero distintos del cero y, por tanto, es un \emph{dominio de integridad no conmutativo}.

\subsection{División}

Al ser $\K[t;\sigma]$ un dominio de integridad no conmutativo, podemos definir algoritmos de división en $\K[t;\sigma]$ a la izquierda y a la derecha, de forma que para cada $f,g \in \K[t;\sigma], g \neq 0$, existen $q,r \in \K[t;\sigma]$ tales que al dividir por la izquierda obtenemos 
$$f = qg + r,$$ y al dividir por la derecha, $$f = gq + r.$$

\begin{algorithm}[h]\label{alg:led}
    \caption{División Euclídea por la Izquierda en $\K[t;\sigma]$}
    \begin{algorithmic}[1]
    \REQUIRE $f,g \in \K[t;\sigma]$ con $g \neq 0$.
    \ENSURE $q,r \in \K[t;\sigma]$ tal que $f = qg + r$ y $\gr(r) <  \gr(g)$.
    \STATE $q \longleftarrow 0$
    \STATE $r \longleftarrow f$
    \WHILE{ $\gr(g) \leq \gr(r)$ }
    \STATE $a \longleftarrow cl(r)\sigma^{\gr(r) - \gr(g)}(cl(g)^{-1})$
    \STATE $q \longleftarrow q + at^{\gr(r) - \gr(g)}$
    \STATE $r\longleftarrow r - at^{\gr(r) - \gr(g)}g$
    \ENDWHILE
    \end{algorithmic}
\end{algorithm}
\phantom{}
\begin{algorithm}[h] \label{alg:red}
    \caption{División Euclídea por la Derecha en $\K[t;\sigma]$}
    \begin{algorithmic}[1]
    \REQUIRE $f,g \in \K[t;\sigma]$ con $g \neq 0$.
    \ENSURE $q,r \in \K[t;\sigma]$ tal que $f = gq + r$ y $\gr(r) <  \gr(g)$.
    \STATE $q \longleftarrow 0$
    \STATE $r \longleftarrow f$
    \WHILE{ $\gr(g) \leq \gr(r)$ }
    \STATE $a \longleftarrow \sigma^{-gr(g)}(cl(g)^{-1}cl(r))$
    \STATE $q \longleftarrow q + at^{\gr(r) - \gr(g)}$
    \STATE $r\longleftarrow r - gat^{\gr(r) - \gr(g)}$
    \ENDWHILE
    \end{algorithmic}
\end{algorithm}

Los polinomios $r$ y $q$ obtenidos como la salida del Algoritmo \ref{alg:led} reciben el nombre de \emph{resto por la izquierda} y \emph{cociente por la izquierda} respectivamente. Usaremos la notación $r = \restoizq(f,g)$ y $q = \cocienteizq(f,g)$. Se asumen las mismas convenciones y notaciones para el algoritmo de la división por la derecha.

De ahora en adelante, para simplificar la notación, llamaremos $R = \K[t,\sigma]$. Como estamos asumiendo que $\sigma$ es un automorfismo y existen los algoritmos de división por la izquierda y por la derecha, $R$ es tanto un DIP por la izquierda como por la derecha y, por tanto, es un DIP.

Sea $I$ un ideal bilátero de $R$, entonces será de la forma $I = Rf = f^*R$ y para cualquier $g \in R$, existen $g',\tilde{g} \in R$ tales que $fg = g'f$ y $gf^* = f^*\tilde{g}$. Los elementos $f$ tales que para cualquier $g \in R$ existen $g',\tilde{g} \in R$ tales que $fg = g'f$ y $gf = f\tilde{g}$ se llaman elementos \emph{biláteros} y además $Rf = fR$ es un ideal.

El siguiente Teorema nos permite determinar los elementos biláteros y, por tanto, los ideales de $R$. Este teorema puede encontrarse en \cite[p. 5]{jacobson1996}.

\begin{teorema}\label{th:js}
Sea $R = \K[t;\sigma]$. Se verifican las siguientes afirmaciones.
\begin{itemize}
    \item[(i)] Los elementos biláteros de $R$ son los elementos $ac(t)t^n$ donde $a \in \K$, $n \in \N_0$ y $c(t) \in \mathcal Z(R)$, siendo $\mathcal Z(R)$ el centro \footnote{El centro de un anillo $\mathcal R$ es el conjunto $\mathcal Z(\mathcal R) = \{c \in  \mathcal R: c r = r c, \ \forall r \in \mathcal R\}$} de $R$.
    \item[(ii)] $\mathcal Z(R) = \text{Inv}(\sigma)$, donde $\text{Inv}(\sigma) = \{ f \in \K : \sigma(d) = d\}.$
    \item[(iii)] Supongamos que $\sigma$ tiene orden finito $n$, es decir, $\sigma^n = Id$. Entonces, el centro $\mathcal Z(R)$ está formado por los polinomios de la forma $$\gamma_0 + \gamma_1t^r + \gamma_2t^{2r} + \cdots \gamma_st^{sr},$$ donde $\gamma_i \in \K$.
\end{itemize}
\end{teorema}

Dados $f,g \in R$, $Rf \subseteq Rg$, $Rg \neq 0$, significa que $g$ es un divisor a la derecha de $f$ y lo denotamos por $g \ |_d \ f$.  Equivalentemente, podemos decir que $f$ es un múltiplo a la izquierda de $g$.

Se tiene que $Ra = Rb \neq 0$ si y solo si $f \ |_d \ g$ y $g \ |_d \ f$. Así, $f = hg$ y $g = sf$ y, por tanto, $g = shg$. Por tanto, $sh = 1$, lo que implica que $hs = 1$ ya que $R$ es un dominio de integridad. Por tanto, $h$ y $s$ son unidades. Se dice entonces que $f$ y $g$ son asociados por la izquierda, en el sentido de que $g = uf, u \in \mathcal U(R)$.

Tenemos que $Rf + Rg = Rh$. Entonces $ h \ |_d \ f$ y $ h \ |_d \ g$. Además, si $e \ |_d \ f$ y $e \ |_d \ g$, entonces $Re \supset Rf$ y $Re \supset Rg$, por lo que $Re \supset Rh$ y $ e \ |_d \ h$. De esta manera, $h$ es un \emph{máximo común divisor por la derecha} de $f$ y $h$ y lo denotamos por $h = (f,g)_r.$

Dados $a,b \neq 0$, se dice que un anillo $\mathcal{A}$ satisface la \emph{condición de Ore-Wedderburn por la izquierda} si $Ra \cap Rb \neq 0$. En \cite[p.4]{jacobson1996} se demuestra que $R$ cumple dicha condición. Tenemos así que $Rf \cap Rg = Rm$, por tanto $m = g'f = f'g \neq 0$. Además, si $f \ |_d \ n$ y $g \ |_d \ n$ entonces $Rm = Rf \cap Rb \supset Rn$, por tanto $m \ |_d \ n$. De esta forma, $m$ es un \emph{mínimo común múltiplo por la izquierda} de $f$ y $g$ y se denota por $[f,g]_\ell$. Tanto el máximo común divisor como el mínimo común múltiplo son únicos, salvo una multiplicación por la izquierda de una unidad.

Tanto $(f,g)_r$ como $[f,g]_\ell$ pueden ser calculados usando una versión apropiada del algoritmo extendido de Euclides por la Izquierda, el cual se detalla explícitamente a continuación.

\begin{algorithm}[h]
    \caption{Algoritmo Extendido de Euclides por la Izquierda (AEEI) en $\K[t;\sigma]$}
    \begin{algorithmic}[1]
    \REQUIRE $f,g \in \K[t;\sigma]$ con $f,g \neq 0$.
    \ENSURE $n \in \N$, $u_i,v_i,q_i,f_i \in \K[t;\sigma]$ tales que $f_i = u_if + v_ig$, $q_i = \cocienteizq(f_{i-1},f_i)$, $f_n = (f,g)_r$, $u_nf = -v_ng = [f,g]_\ell$ para $1 \leq i \leq n + 1$. 
    \STATE $u_0,v_1 \longleftarrow 1$
    \STATE $u_1,v_0 \longleftarrow 1$
    \STATE $f_0 \longleftarrow f$
    \STATE $f_1 \longleftarrow g$
    \STATE $i \longleftarrow 1$
    \WHILE{ $f_i \neq 0$ }
    \STATE $q_i \longleftarrow \cocienteizq(f_{i-1},f_i)$
    \STATE $u_{i+1} \longleftarrow u_{i-1} - q_iu_i$
    \STATE $v_{i+1} \longleftarrow v_{i-1} - q_iv_i$
    \STATE $f_{i+1} \longleftarrow f_{i-1} - q_if_i$
    \STATE $n \longleftarrow i$
    \STATE $i \longleftarrow i + 1$

    \ENDWHILE
    \end{algorithmic}
\end{algorithm}

De igual manera, tanto $(f,g)_\ell$ como $[f,g]_r$ se pueden calcular utilizando una versión del Algoritmo Extendido de Euclides por la Derecha. 

\begin{algorithm}[h] 
    \caption{Algoritmo Extendido de Euclides por la Derecha (AEED) en $\K[t;\sigma]$} \label{alg:eucld}
    \begin{algorithmic}
    \REQUIRE $f,g \in \K[t;\sigma]$ con $f,g \neq 0$.
    \ENSURE $\{u_i,v_i,r_i\}_{i=0,\dots,h,h + 1}$ tales que $r_i = fu_i + gv_i$ para cualquier $i \in \{0,\dots,h,h+1\}$, \\ $r_h = (f,g)_\ell$ y $u_{h+1}f = [f,g]_r$.
    \STATE $u_0,v_1 \longleftarrow 1$
    \STATE $u_1,v_0 \longleftarrow 1$
    \STATE $r_0 \longleftarrow f,r_1 \longleftarrow g$
    \STATE $q \longleftarrow 0, \text{rem} \longleftarrow 0$
    \STATE $i \longleftarrow 1$
    \WHILE{ $r_i \neq 0$ }
    \STATE $q,\text{rem} \longleftarrow \text{rquo-rem}(r_{i-1},r_i)$
    \STATE $r_{i+1} \longleftarrow \text{rem}$
    \STATE $u_{i+1} \longleftarrow u_{i-1} - u_iq_i$
    \STATE $v_{i+1} \longleftarrow v_{i-1} - v_iq_i$
    \STATE $i \longleftarrow i + 1$
    \ENDWHILE
    \end{algorithmic}
\end{algorithm}


\begin{lema}\label{lema:24}
Sean $f,g \in \K[t;\sigma]$ y $\{u_i,v_i,r_i\}_{i=0,\dots,h}$ los coeficientes obtenidos tras aplicar el AEED \ref{alg:eucld} a los polinomios sesgados $f$ y $g$. Denotamos $R_0 = \begin{bmatrix}
u_0 & u_1 \\
v_0 & v_1 \\ 
\end{bmatrix}$, $Q_i = \begin{bmatrix}
    0 & 1 \\
    1 & -q_i
\end{bmatrix}$ y $R_i = R_0Q_1\cdots Q_i$ para $i \in \{0,\dots,h\}$. Entonces, para cualquier  $i \in \{0,\dots,h\}$ se verifican las siguientes afirmaciones:

\begin{itemize}
    \item[(i)] $[f \ g]R_i = [r_{i-1} \ r_i]$.
    \item[(ii)] $R_i = \begin{bmatrix}
        u_i & u_{i+1} \\
        v_i & v_{i+1}
    \end{bmatrix}$. 
    \item[(iii)] $fu_i + gv_i = r_i$.
    \item[(iv)] $R_i$ tiene inversa por la izquierda y por la derecha.
    \item[(v)] $(u_i,v_i)_r = 1$.
    \item[(vi)] $\gr(f) = \gr(r_{i-1}) + \gr(v_i) \quad (i \geq 1)$. 
\end{itemize}

\begin{proof}
Para probar \textit{(i)} vamos a proceder por inducción. El caso $i = 0$ es trivial, ya que  $[f \ g]R_0 = [f \ g]\begin{bmatrix}
    u_0 & u_1 \\
    v_0 & v_1 \\ 
    \end{bmatrix} = [fu_0 + gv_0 \ \ fu_1 + gv_1] = [r_0 \ r_1].$

Supongámoslo cierto para $n$ y probémoslo para $n+1$.

\begin{equation}
[f \ g]R_{n+1} = [f \ g]R_0Q_1\cdots Q_nQ_{n+1} = [f \ g]R_nQ_{n+1} = [r_n \ r_{n+1}]Q_{n+1}.
\end{equation}

Usando que $Q_{n+1} = \begin{bmatrix}
    0 & 1 \\
    1 & -q_{n+1}
\end{bmatrix}$, obtenemos que $[r_n \ r_{n+1}]Q_{n+1} = \begin{bmatrix}
    r_{n+1} & r_n - r_{n+1}q_{n+1} 
\end{bmatrix}$.

Y por definición, $r_{n+2} = r_n - r_{n+1}q_{n+1}$, obteniéndose lo que buscábamos.

Para \textit{(ii)}, procedemos de forma similar utilizando inducción.

El caso $i = 0$ se tiene por definición. Supongámoslo cierto para $n$ y probémoslo para $n+1$.

\begin{equation}
R_{n+1} = R_nQ_{n+1} = \begin{bmatrix}
    u_n & u_{n+1} \\
    v_n & v_{n+1}
\end{bmatrix}\begin{bmatrix}
    0 & 1 \\
    1 & -q_{n+1}
\end{bmatrix} = \begin{bmatrix}
    u_{n+1} & u_n - u_{n+1}q_{n+1} \\
    v_{n+1} & v_n - v_{n+1}q_{n+1} 
\end{bmatrix} = \begin{bmatrix}
    u_{n+1} & u_{n+2} \\
    v_{n+1} & v_{n+2}
\end{bmatrix} .
\end{equation}

La afirmación \textit{(iii)} se sigue de \textit{(i)} y \textit{(ii)}.

Para \textit{(iv)}, observemos que $T_i = \begin{bmatrix}
    q_i & 1 \\
    1 & 0
\end{bmatrix}$ es una inversa por la izquierda y por la derecha de $Q_i$. Por tanto, $S_i = T_i \cdots T_1R_0$ es una inversa por la izquierda y por la derecha de $R_i$.

Para \textit{(v)}, si $R_i^{-1} = S_i = \begin{bmatrix}
    a & b \\
    c & d 
\end{bmatrix}$, entonces

$$ \begin{bmatrix}
    a & b \\
    c & d 
\end{bmatrix}\begin{bmatrix}
u_i & u_{i+1} \\
v_i & v_{i+1}
\end{bmatrix} = \begin{bmatrix}
    1 & 0 \\
    0 & 1
    \end{bmatrix}, $$

    por tanto, existen $a,b \in \K[t;\sigma]$ verificando $au_i + bv_i = 1$ y entonces $(u_i,v_i)_r = 1$.

Por último, vamos a probar la afirmación \textit{(vi)} por inducción.

Para $i = 1$, $r_0 = f$ , $v_1 = 1$ y la igualdad es trivial.

Observemos que por el propio procedimiento del algoritmo $\gr(r_i) < \gr(r_{i-1})$ y $\gr(v_{i-1}) < \gr(v_i)$. Además, puesto que $r_{i+1} = r_{i-1} - r_iq_i$ y $v_{i+1} = v_{i-1} - v_iq_i$ para cualquier $i$, se tiene que $\gr(r_{i-1}) = \gr(r_i) + \gr(q_i)$ y $\gr(v_{i-1}) = \gr(v_i) + \gr(v_i)$ para cualquier $i$. Ahora, por la hipótesis de inducción, $\gr(f) = \gr(r_{i-1}) + \gr(v_i) = \gr(r_i) + \gr(q_i) + \gr(v_{i+1}) - \gr(q_i) = \gr(r_i) + \gr(v_{i+1})$, como queríamos. 

\end{proof}



\end{lema}

\subsection{Norma}

El siguiente concepto (ver \cite{Norm}) nos será útil a la hora de calcular los restos obtenidos al dividir polinomios sesgados.

\begin{definicion}
Sea $\gamma \in \K$, la \emph{$j$-ésima} norma de $\gamma$ es 
$$N_j(\gamma) = \gamma \sigma(\gamma) \cdots \sigma^{j-1}(\gamma), \quad j > 0, \quad N_0(\gamma) = 1.$$
\end{definicion}

La noción de $j$-norma también admite una versión con índices negativos dada por $$N_{-j}(\gamma) = \gamma \sigma^{-1}(\gamma) \cdots \sigma^{-j+1}(\gamma).$$

\begin{proposicion}\label{prop:norma}
Sea $\gamma \in \K$ y $g(x) = \sum_{i=0}^{r}g_ix_i \in R$. Entonces se verifican las siguientes afirmaciones.
\begin{itemize}
    \item[(i)] El resto por la izquierda de la división por la izquierda de $g$ por $x - \gamma$ es $\sum_{i=0}^{r}g_iN_i(\gamma)$.
    \item[(ii)] El resto por la derecha de la división por la derecha de $g$ por $x - \gamma$ es $\sum_{i=0}^{r}\sigma^{-i}(g_i)N_{-i}(\gamma)$.
    \item[(iii)] $N_j(\sigma^k(\gamma)) = \sigma^k(N_j(\gamma))$ para cualquier $j,k \in \N$. 
\end{itemize}
\end{proposicion}

\begin{proof}
Para probar \textit{(i)} procedemos como en \cite[Lema 2.4]{Norm}.

Vamos a demostrar en primer lugar por inducción la siguiente proposición:

\begin{equation} \label{prop:aux1}
    \text{Para todo } n \geq 0, \quad x^n - N_n(\gamma) \in R\cdot(x-\gamma).
\end{equation}

El caso base es evidente, ya que $x^0 - N_0(\gamma) = 0 \in R\cdot(x - \gamma)$. Supongámoslo cierto para $n$.

Para $n + 1$ tenemos que

\begin{align*}
    x^{n+1} - N_{n+1}(\gamma) &= x^{n+1} - \sigma(N_n(\gamma))(\gamma) \\
                              &= x^{n+1} - \sigma(N_n(\gamma))(\gamma) + \sigma(N_n(\gamma))x - \sigma(N_n(\gamma))x \\
                              &= x^{n+1} + \sigma(N_n(\gamma))(x-\gamma) - \sigma(N_n(\gamma))x \\
                              &= \sigma(N_n(\gamma))(x-\gamma) + xx^n -xN_n(\gamma) \\
                              &=  \sigma(N_n(\gamma))(x-\gamma) + x(x^n - N_n(\gamma)) \in R\cdot(x-\gamma).
\end{align*}


Tenemos que si $x - \gamma$ divide por la izquierda a $g(x)$, entonces $g(x) = q(x)(x-\gamma) + r$, observemos que utilizando la proposición \eqref{prop:aux1}

$$g(x) - \sum_{i=0}^{r} g_iN_i(\gamma) = \sum_{i=0}^{r}g_ix^i - \sum_{i=0}^{r} g_iN_i(\gamma) =  \sum_{i=0}^{r} g_i(x^i - N_i(\gamma))  \in R\cdot(x-\gamma),$$

y, por tanto, $r = \sum_{i=0}^{r} g_iN_i(\gamma)$.

Para \textit{(ii)} procedemos de manera análoga.

Probaremos por inducción que 

\begin{equation} \label{prop:aux2}
    \text{Para todo } n \geq 0, \quad x^n - N_{-n}(\gamma) \in (x-\gamma)\cdot R.
\end{equation}

El caso base es de nuevo trivial, pues $x^0 - N_0(\gamma) = 0 \in (x-\gamma)\cdot R.$

Para $n + 1$ tenemos que

\begin{align*}
    x^{n+1} - N_{-n-1}(\gamma) &= x^{n+1} - \gamma\sigma^{-1}(N_{-n}(\gamma)) \\
                              &= x^{n+1} - \gamma\sigma^{-1}(N_{-n}(\gamma)) + x\sigma^{-1}(N_{-n}(\gamma)) - x\sigma^{-1}(N_{-n}(\gamma)) \\
                              &= x^{n+1} + (x-\gamma)\sigma^{-1}(N_{-n}(\gamma)) - x\sigma^{-1}(N_{-n}(\gamma)) \\
                              &= (x-\gamma)\sigma^{-1}(N_{-n}(\gamma)) + x^nx -N_{-n}(\gamma)x \\
                              &=  (x-\gamma)\sigma^{-1}(N_{-n}(\gamma)) + (x^n - N_{-n}(\gamma))x \in (x-\gamma)\cdot R.
\end{align*}

Donde se ha utilizado que para todo $a \in \mathbb{K}$, se tiene que $ax = x\sigma^{-1}(a)$ en $R$.

De esta forma, observamos que si $x - \gamma$ divide por la derecha a $g(x)$, entonces \\ $g(x) = (x-\gamma)q(x) + r$. Utilizando la proposición \eqref{prop:aux2}

\begin{align*}
g(x) - \sum_{i=0}^{r}\sigma^{-i}(g_i)N_{-i}(\gamma) &= \sum_{i=0}^{r}g_ix^i - \sum_{i=0}^{r}\sigma^{-i}(g_i)N_{-i}(\gamma) = \sum_{i=0}^{r}x^i\sigma^{-i}(g_i) - \sum_{i=0}^{r}\sigma^{-i}(g_i)N_{-i}(\gamma) \\
                                                    &= \sum_{i=0}^{r}(x^i - N_{-i}(\gamma))\sigma^{-i}(g_i) \in (x-\gamma)\cdot R
\end{align*}

y, por tanto, $r = \sum_{i=0}^{r}\sigma^{-i}(g_i)N_{-i}(\gamma)$.

Para probar \textit{(iii)} tenemos que

\begin{align*}
N_j(\sigma^k(\gamma)) &= \sigma^k(\gamma)\sigma^{k+1}(\gamma)\cdots\sigma^{k + j - 1}(\gamma) \\
                      &= \sigma^k(\gamma\sigma(\gamma)\cdots\sigma^{j-1}(\gamma)) \\
                      &= \sigma^k(N_j(\gamma)).
\end{align*}

\end{proof}

\section{Ciclicidad en códigos convolucionales}

En esta sección estudiaremos los códigos convolucionales cíclicos sesgados, los cuales se definen mediante un anillo de polinomios sesgados del cuerpo de fracciones $\F(t)$.

Comenzaremos con un resumen de algunos intentos previos para definir los códigos convolucionales cíclicos.

\subsection{Primeros enfoques}

Recordemos que un código de bloque $\C \subseteq \F^n$ se dice cíclico si es invariante bajo un desplazamiento cíclico, es decir, si 

\begin{equation}\label{eq:inv} (v_0,v_1,\dots,v_{n-1}) \in \C \Rightarrow (v_{n-1},v_0,\dots,v_{n-2}) \in \C,\end{equation}

o, equivalentemente, si $\C S \subseteq \C$, donde 

\begin{equation}\label{eq:matrixS} S = \left[ \begin{array}{cccc}
    0 & 1 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & 1 \\
    1 & 0 & \cdots & 0 \\
\end{array} \right] \in \mathcal{M}_{n}(\F).\end{equation}

Para los códigos cíclicos de bloque utilizamos su representación polinomial, la cual está basada en el $\F$-isomorfismo
$$\mathfrak{p}: \F^n \longrightarrow \mathcal{R}_n, \quad v = (v_0,v_1,\dots,v_{n-1}) \mapsto \mathfrak{p}(v) = \sum_{i=0}^{n-1}v_ix^i,$$
donde $\mathcal{R}_n = \F[x]/\langle x^n - 1 \rangle$.

De esta forma, la aplicación $\mathfrak{p}$ nos permite representar un desplazamiento cíclico multiplicando por $x$. Como consecuencia, un código cíclico de bloque $\C$ puede ser representado como un ideal $\mathfrak{p}(\C)$ en $\mathcal{R}_n$, de esta forma, un código de bloque $\C$ es cíclico si y solo si $$a \in \mathfrak{p}(\C) \Rightarrow xa \in \mathfrak{p}(\C).$$

Por tanto, podríamos pensar en definir los códigos convolucionales cíclicos de manera parecida. Sea

$$\mathfrak{p}: \F[z]^n \longrightarrow \mathcal{R}_n[z], \quad \sum_{i \geq 0} z^iv_i \mapsto \sum_{i \geq 0}z^i\mathfrak{p}(v_i),$$

un isomorfismo de $\F[z]$-módulos por la izquierda, con inversa $\mathfrak{v} := \mathfrak{p}^{-1}$.

Sería natural definir la ciclicidad de los códigos convolucionales al igual que con los códigos de bloque, es decir, exigiendo invarianza bajo desplazamientos cíclicos. Sin embargo, la siguiente proposición nos hace rechazar automáticamente esta idea.

\begin{proposicion}\cite[p. 194]{cccheide}\label{prop:ccc}
Sea $\C \subseteq \F[z]^n$ un código que satisface \eqref{eq:inv} para todo $(v_0,\dots,v_{n-1}) \in \C$. Entonces $\C$ es un código de bloque.
\end{proposicion}

\begin{proof}
Por suposición, $\C S \subseteq \C$, donde $S$ es una matriz como en \eqref{eq:matrixS}. El polinomio minimal de $S$ está dado por $x^n - 1$. Sea $x^n - 1 = \pi_1 \cdots \pi_r$ una factorización en factores irreducibles distintos en $\F$ (la cual existe según el Teorema \ref{th:fac}). De esta forma, obtenemos la descomposición

$$\F[z]^n = \ker \pi_1(S) \oplus \ker \pi_2(S) \oplus \cdots \oplus \ker \pi_r(S)$$

de $\F[z]^n$ en $\F[z]$-submódulos los cuales son sumandos directos $S$-invariantes minimales. Pero recordemos que $\C$ también es un sumando directo, por tanto, obtenemos que 

$$\C = \bigoplus_{i \in T}\ker \pi_i(S), \quad \text{donde } T = \{i : \ker \pi_i(S) \cap \C \neq \{0\}\}.$$

Puesto que $\F^nS = \F^n$, los $\F[z]$-submódulos $\ker \pi_i(S)$ están generados por $\ker \pi_i(S) \cap \F^n$ lo que nos lleva directamente a una matriz generadora constante, y, por tanto, a un codificador constante para $\C$. De esta manera, en base a la definición \ref{def:complejidad}, $\C$ es un código de bloque ya que su complejidad es $0$.
\end{proof}

El resultado de la Proposición \ref{prop:ccc} llevó a Piret \cite{Piret} a dar una noción más general y compleja de ciclicidad en códigos convolucionales. En vez de una invarianza por desplazamientos de $\C$ bajo una matriz de desplazamiento $S$ \eqref{eq:matrixS}, Piret introdujo un tipo diferente de ciclicidad y llamó a un código convolucional $\C$ cíclico si existía un $m \in \N$, coprimo con la longitud $n$ del código tal que 

\begin{equation}
\sum_{i \geq 0} z^v v_i \in \C \Rightarrow \sum_{i \geq 0}z^iv_iS^{(m^i)} \in \C .
\end{equation}

En forma polinomial, es decir, en el anillo de polinomios $\mathcal{R}_n[z]$, esto significa que

\begin{equation}
    \sum_{i \geq 0} z^i\mathfrak{p}(v_i) \in \mathfrak{p}(\C) \Rightarrow \sum_{i \geq 0}z^ix^{(m^i)}\mathfrak{p}(v_i) \in \mathfrak{p}(\C). 
\end{equation}

El hecho de que $m$ y $n$ sean coprimos garantiza, no solo que el polinomio minimal de $S^m$ es el mismo que el de $S$, es decir, $x^n - 1$, sino que también que la aplicación $x \longmapsto x^m$ induce un $\F$-automorfismo $\sigma$ de $\mathcal{R}_n$.

La noción de ciclicidad de Piret fue generalizada por Roos \cite{Roos} de una forma natural, considerando $\F$-automorfismos $\sigma$ de $\mathcal{R}_n$ arbitrarios utilizando extensiones de Ore.

\begin{definicion}
Un $\sigma$-código convolucional cíclico $\C$ ($\sigma$-CCC) es un sumando directo de $\F[z]^n$ tal que es un ideal por la izquierda de $\mathcal{R}_n[z;\sigma]$.
\end{definicion}

Observamos que $\mathcal{R}_n[z;\sigma]$ puede tener varios divisores de $0$ y, por tanto, no es un anillo de división (ya que $\mathcal{R}_n$ no es un cuerpo). Tampoco es un DIP, por lo que no disponemos de operaciones como la división euclídea, el mínimo común múltiplo o el máximo común divisor. Como consecuencia, la decodificación utilizando estructuras cíclicas puede llegar a ser bastante complicada.


\subsection{Códigos convolucionales cíclicos sesgados}

Hemos visto que la noción clásica de ciclicidad para los códigos de bloque no sirve para los códigos convolucionales. También se ha visto que utilizar la extensión de Ore del anillo $\mathcal{R}_n[z]$ puede tener ciertos problemas, ya que no disponemos de las herramientas matemáticas desarrolladas para las extensiones de Ore cuando estas son un dominio de ideales principales.

Recordemos que los códigos convolucionales podían ser vistos como subespacios vectoriales $k$-dimensionales del cuerpo de fracciones $\F(t)^n$, siendo $\F$ un cuerpo finito arbitrario. Dado un $\F(t)$-automorfismo $\sigma$, consideramos su extensión de Ore $\F(t)[x;\sigma]$. Al ser $\F(t)$ un cuerpo, $\F(t)[x;\sigma]$ es un DIP y todos los algoritmos detallados en la sección \ref{sec:ore} son válidos.

El grupo $\text{Aut}(\F(t))$ es isomorfo a $\text{PGL}(2,\F)$, el grupo lineal proyectivo de las matrices $2 \times 2$ sobre $\F$ \cite[p. 198]{van1949modern}, por tanto, es un grupo finito. En particular, $\sigma$ tiene un orden finito $n_\sigma$ bajo composición, es decir, $\sigma^{n_\sigma} = Id$,$\ \forall \sigma \in \text{Aut}(\F(t)), n_\sigma \in \N$.

Nuestro objetivo es describir los códigos convolucionales cíclicos como un subespacio de $\F(t)^n$ asociado a un ideal por la izquierda del anillo cociente $$\mathcal{R}_n = \F(t)[x;\sigma]/\langle x^n - 1 \rangle.$$
En efecto, cada ideal por la izquierda $I \leq \R_n$ nos da un código convolucional $\C = \mathfrak{v}(I)$ de longitud $n$, donde $\mathfrak{v} : \R_n \mapsto \F(t)^n$ es la biyección asociada a la base $\mathcal{B} = \{1,x,\dots,x^{n-1}\}$, $\mathfrak{v}^{-1} = \mathfrak{p}$. De esta forma, la longitud de los códigos convolucionales coincide con el orden de $\sigma$.


\begin{definicion}

Un \emph{código convolucional cíclico sesgado (SCCC)} $\C$ es un código convolucional cuya imagen $\mathfrak{p}(\C)$ bajo

$$\F(t)^n \xlongrightarrow{\mathfrak{p}} \frac{\F(t)[x;\sigma]}{\langle x^n - 1 \rangle}$$ es un ideal por la izquierda.
\end{definicion}

La estructura algebraica de los SCCC es muy diferente de la de los códigos cíclicos de bloque. Estos últimos son ideales del anillo cociente $\F[x]/\langle x^n - 1 \rangle$ siempre y cuando $n$ y la característica de $\F$ sean coprimos. El Teorema \ref{th:ci} nos dice que existe una correspondencia biyectiva entre los códigos cíclicos y los divisores mónicos del polinomio $x^n -1$. Debemos estudiar la estructura de $\R_n$ para poder entender los SCCCs. El Teorema \ref{th:js} nos dice que el centro de $\Ft$ es el anillo de polinomios conmutativo $\F(t)^{\sigma}[x^n]$, donde $\F(t)^{\sigma}$ denota el subcuerpo fijo, es decir, los elementos $a \in \F(t)$ tales que $\sigma(a) = a$.

\begin{teorema} \label{th:1}
El anillo $\R_n$ es isomorfo al anillo de matrices $\mathcal{M}_n(\F(t)^{\sigma}).$
\end{teorema}

\begin{proof}
    Puesto que el polinomio $x^n - 1$ es irreducible en $\F(t)^{\sigma}[x^n]$ (de hecho es lineal), tenemos que $\langle x^n - 1 \rangle$ es un ideal bilátero primo de $R = \Ft$. Por tanto, $R_n$ es un anillo Artiniano simple por \cite[Th. 13, p. 40]{jacobson1943theory}. Por el Teorema de Artin-Wedderburn (ver \cite[Th 1.1]{artin}), obtenemos que $\R_n \cong \mathcal{M}_n(D)$, donde $D$ es el álgebra de división formada por el conjunto de endomorfismos de un módulo por la izquierda simple sobre $\R_n$. Puesto que $x - 1$ es un factor irreducible por la derecha de $x^n -1$ en $R$, tomamos $M = R/R(x-1)$. El endomorfismo de anillos de $M$ es isomorfo a $\F(t)^{\sigma}$. En efecto, un isomorfismo de anillos explícito se puede definir de la siguiente forma. 

    Dado un endomorfismo $\R_n$-lineal $\ f : M \mapsto M$, escribimos $f(1 + R(x-1)) = \psi(f) + R(x-1)$, donde $\psi(f) \in \F(t)$ está determinado de manera única. Unos cálculos sencillos muestran que $\psi(f) \in \F(t)^{\sigma}$, y que la asignación $ f \mapsto \psi$ es el isomorfismo buscado. Con este isomorfismo, se tiene que $\R_n \cong \mathcal{M}_n(\F(t)^\sigma).$
\end{proof}

Como consecuencia del teorema anterior, para cada $n$ existe un SCCC de dimensión $k \leq n$.

Estas estructuras cíclicas sobre los códigos convolucionales presentan muchas ventajas computaciones, ya que al trabajar en la extensión de Ore de un cuerpo disponemos de muchas herramientas aritméticas.

Veamos un ejemplo de SCCC.

\begin{ejemplo}\label{ej:sccc}
Sea $\sigma : \F_2(t) \rightarrow \F_2(t)$ un automorfismo de orden $2$ definido por $\sigma(t) = 1/t$. Observamos que $x^2 + 1 = (x + 1/t^2)(x + t^2)$ en la extensión de Ore $\F_2(t)[x;\sigma]$. Por tanto, el ideal por la izquierda generado por $g = x + t^2$ es un $\F_2(t)$-subespacio propio de dimensión uno de $\R = \F_2(t)[x;\sigma]/\langle x^2 - 1 \rangle$. De hecho, $\C = \mathfrak{v}(\R g)$ es un SCCC cuya matriz generadora es 
$$
\begin{bmatrix}
    t^2 & 1 \\
    1 & \frac{1}{t^2}
\end{bmatrix}
$$ 
,ya que $1(t^2 + x) = t^2 + x$ y $x(t^2 + x) = xt^2 + x^2 \equiv 1 + (1/t^2)x \mod x^2 - 1.$

\subsection{Construcción de SCCCs}

$\mathcal{R}_n$ es un DIP y, por tanto, todo ideal por la izquierda es principal. De esta manera, cualquier SCCC está generado por un divisor a la derecha de $x^n - 1$. Por tanto, al igual que con los códigos de bloque, debemos encontrar divisores por la derecha de este polinomio. Si estos factores pertenecen a $\F[x]$, los códigos construidos serán códigos cíclicos de bloque. Al ser $\R_n$ no conmutativo, existen descomposiciones de $x^n - 1$ con coeficientes no constantes en $\F(t)$ (ver \ref{ej:sccc}). El problema es que hasta ahora no se ha descubierto un algoritmo de factorización eficiente para polinomios de Ore sobre $\F(t)$. Por tanto, vamos a proporcionar un procedimiento específico para $x^n-1$. Empecemos con un método para encontrar divisores a la derecha lineales.

\begin{proposicion}\label{prop:c}
Sea $\beta \in \F(t)$, entonces $x - \beta$ divide por la derecha a $x^n - 1$ si y solo si $\beta = \sigma(c)c^{-1}$, para algún $c \in \F(t)$ no nulo.
\end{proposicion}

\begin{proof}
Sea $R = \F(t)[x;\sigma]$. Si $x - \beta$ divide por la derecha a $x^n -1$, entonces $R/R(x - \beta) \cong R/R(x-1)$ como $\R$-módulos a la izquierda y $\beta = \sigma(c)c^{-1}$ para algún $c \in \F(t)$ no nulo \cite[Proposición 2.4]{modulos}. 
Recíprocamente, dado $c \in \F(t)$ no nulo y $\beta = \sigma(c)c^{-1}$, se puede deducir una versión de \cite[Lema 4]{Chaussade} para polinomios sesgados sobre $\F(t)$ de \cite[Proposición 1.3.11]{jacobson1996} usando que $\sigma^i(c)c^{-1} = N_i(\sigma(c)c^{-1})$, lo que nos da que $x - \beta$ es un divisor a la derecha de $x^n - 1$.
\end{proof}

El Teorema \ref{th:1} nos dice que $x^n -1$ se puede descomponer como el mínimo común múltiplo por la izquierda de polinomios lineales (correspondientes a la representación del ideal cero de $\R_n$ como intersección de $n$ módulos por la izquierda maximales). Para encontrar esta descomposición de $x^n -1$ (y, por tanto, los divisores a la derecha no lineales), nuestra estrategia será calcular $\beta \in \F(t)$ de manera que 

\begin{equation}\label{eq:beta}
[x - \beta,x - \sigma(\beta),\dots,x - \sigma^{n-1}(\beta)]_\ell = x^n - 1
\end{equation}

No todo $\beta \in \F(t) \backslash \F$ satisface \eqref{eq:beta}. Usando \cite[Proposición 1]{boucher2007}, \eqref{eq:beta} se satisface si y solo si el determinante de

\[
\begin{bmatrix}
1 & \beta & \beta\sigma(\beta) & \cdots & \beta\sigma(\beta)\cdots\sigma^{n-2}(\beta) \\
1 & \sigma(\beta) & \sigma(\beta)\sigma^2(\beta) & \cdots & \sigma(\beta)\sigma^{2}(\beta)\cdots\sigma^{n-1}(\beta) \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & \sigma^{n-1}(\beta) & \sigma^{n-1}(\beta)\beta & \cdots & \sigma^{n-1}(\beta)\beta\cdots\sigma^{n-3}(\beta)
\end{bmatrix}
\]

no es cero. Usando la Proposición \ref{prop:c}, $\beta = \sigma(c)c^{-1}$, para un $c \in \F(t)$ no nulo. De esta forma, sustituyendo obtenemos 

\[
\begin{vmatrix}
1 & \sigma(c)c^{-1} & \sigma(c)c^{-1}\sigma(\sigma(c)c^{-1}) & \cdots & \sigma(c)c^{-1}\sigma(\sigma(c)c^{-1})\cdots\sigma^{n-2}(\sigma(c)c^{-1}) \\
1 & \sigma(\sigma(c)c^{-1}) & \sigma(\sigma(c)c^{-1})\sigma^2(\sigma(c)c^{-1}) & \cdots & \sigma(\sigma(c)c^{-1})\sigma^{2}(\sigma(c)c^{-1})\cdots\sigma^{n-1}(\sigma(c)c^{-1}) \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & \sigma^{n-1}(\sigma(c)c^{-1}) & \sigma^{n-1}(\sigma(c)c^{-1})\sigma(c)c^{-1} & \cdots & \sigma^{n-1}(\sigma(c)c^{-1})\sigma(c)c^{-1}\cdots\sigma^{n-3}(\sigma(c)c^{-1})
\end{vmatrix}
= \] 

\[ = 
\begin{vmatrix}
1 & \sigma(c)c^{-1} & \sigma^2(c)c^{-1} & \cdots & \sigma^{n-1}(c)c^{-1} \\
1 & \sigma^2(c)\sigma(c)^{-1} & \sigma^3(c)\sigma(c)^{-1} & \cdots & c\sigma(c)^{-1}\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & c\sigma^{n-1}(c)^{-1} & \sigma(c)\sigma^{n-1}(c)^{-1} & \cdots & \sigma^{n-2}(c)\sigma^{n-1}(c)^{-1}
\end{vmatrix}
\neq 0  \Leftrightarrow\] 

\[ \Leftrightarrow
\begin{vmatrix}
c & \sigma(c) & \sigma^2(c) & \cdots & \sigma^{n-1}(c) \\
\sigma(c) & \sigma^2(c) & \sigma^3(c) & \cdots & c\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\sigma^{n-1}(c) & c & \sigma(c) & \cdots & \sigma^{n-2}(c)
\end{vmatrix}
\neq 0\]

Equivalentemente, $\{c,\sigma(c),\dots,\sigma^{n-1}(c)\}$ es una base normal de la extensión de cuerpos $\F(t)^{\sigma} \subseteq \F(t)$. El subcuerpo $\F(t)^\sigma = \F(s)$, donde $s \in \F(t)$ se puede calcular como se muestra en \cite[Algoritmo 1]{Gutierrez2006}. En nuestro caso particular, el grupo elegido es el grupo cíclico $\{1,\sigma,\dots,\sigma^{n-1}\}$, por tanto, el elemento $s \in \F(t)$ se puede obtener eligiendo cualquier coeficiente no constante del polinomio conmutativo $\prod_{i=0}^{n-1}(y - \sigma^i(t))$ en $\F(t)[y]$. La existencia de $c \in \F(t)$ para la construcción de dicha base está asegurada por el Teorema de la Base Normal. Realmente, $c$ es un vector cíclico de $\sigma$ visto como una aplicación $\F(t)^{\sigma}$-lineal y una forma de calcularlo es seguir el proceso descrito en \cite[pp. 196-197 y pp. 293-294]{jacobson2012basic}, aunque en la práctica, \emph{una búsqueda aleatoria} será suficiente. Diferentes elecciones de $\beta$ producen descomposiciones distintas de $x^n - 1$. De esta forma, para construir códigos SCCC de una dimensión dada, debemos calcular $\beta = \sigma(c)c^{-1}, c \in \F(t)$, de manera que el determinante de la matriz

\[ 
\begin{bmatrix}
c & \sigma(c) & \sigma^2(c) & \cdots & \sigma^{n-1}(c) \\
\sigma(c) & \sigma^2(c) & \sigma^3(c) & \cdots & c\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\sigma^{n-1}(c) & c & \sigma(c) & \cdots & \sigma^{n-2}(c)
\end{bmatrix}
\]

sea distinto de $0$. De esta forma se verifica \eqref{eq:beta} y basta con tomar cualquier subconjunto $\{i_1,i_2,\dots,i_k\} \subset \{0,1,\dots,n-1\}$ y definir 
$$ f = [x - \sigma^{i_1}(\beta),x - \sigma^{i_2}(\beta),\dots,x - \sigma^{i_k}(\beta)]_\ell.$$

La imagen bajo $\mathfrak{v}$ del ideal por la izquierda generado por $f$ es un SCCC de dimensión $n - k$ y longitud $n$.
\end{ejemplo}

\subsection{Códigos convolucionales sesgados Reed-Solomon}

Utilizando la construcción desarrollada en la sección anterior, vamos a construir códigos convolucionales cíclicos sesgados con una cierta distancia designada de Hamming. Debido a la analogía con los códigos de bloque Reed-Solomon definidos en \ref{subsec:rs}, estos códigos reciben el nombre de \emph{códigos convolucionales sesgados Reed-Solomon (RS)}. 

El siguiente lema es un caso particular de \cite[Corolario 4.13]{lam1988vandermonde} y lo usaremos bastante a lo largo de esta sección.

\begin{lema}\label{lema:sug}
Sea $L$ un cuerpo, $\sigma$ un automorfismo de $L$ de orden finito $n$, y $K = L^\sigma$ el subcuerpo fijo bajo $\sigma$. Sea $\{\alpha_0,\dots,\alpha_{n-1}\}$ una $K$-base de $L$. Entonces, para todo $t \leq n$ y todo subconjunto $\{k_0 < k_1 < \cdots k_{t-1}\} \subseteq \{0,1,\dots,n-1\}$, el determinante de la matriz 

\[
\begin{bmatrix}
\alpha_{k_0} & \alpha_{k_1} & \cdots & \alpha_{k_{t-1}} \\
\sigma(\alpha_{k_0}) & \sigma(\alpha_{k_1}) & \cdots & \sigma(\alpha_{k_{t-1}}) \\
\vdots & \vdots & \ddots & \vdots \\
\sigma^{t-1}(\alpha_{k_0}) & \sigma^{t-1}(\alpha_{k_1}) & \cdots & \sigma^{t-1}(\alpha_{k_{t-1}})
\end{bmatrix}
\]
es distinto de $0$.
\end{lema}

\begin{proof}
Lo probaremos por inducción sobre $t$. El caso $t = 1$ es evidente. Suponemos que el lema se satisface para $t \geq 1$. Debemos de verificar que cualquier matriz 

\[ \Delta = 
\begin{bmatrix}
\alpha_{k_0} & \alpha_{k_1} & \cdots & \alpha_{k_{t}} \\
\sigma(\alpha_{k_0}) & \sigma(\alpha_{k}) & \cdots & \sigma(\alpha_{k_{t}}) \\
\vdots & \vdots & \ddots & \vdots \\
\sigma^{t}(\alpha_{k_0}) & \sigma^{t}(\alpha_{k}) & \cdots & \sigma^{t}(\alpha_{k_{t}})
\end{bmatrix}
\]

de dimensión $(t + 1) \times (t+1)$ tiene un determinante $|\Delta| \neq 0.$

Supongamos que $|\Delta| = 0$, por hipótesis de inducción, las primeras $t$ columnas de $\Delta$ son linealmente independientes y, por tanto existen $a_0,a_1,\dots,a_{t-1} \in L$ tales que

\begin{equation}
(\alpha_{k_t},\sigma(\alpha_{k_t}),\dots,\sigma^t(\alpha_{k_t})) = \sum_{j=0}^{t-1}a_j(\alpha_{k_j},\sigma(\alpha_{k_j}),\dots,\sigma^t(\alpha_{k_j})).
\end{equation}

Es decir, $a_0,\dots,a_{t-1}$ satisfacen el sistema lineal 

\begin{equation}\label{eq:sistemaec}
    \begin{cases}
    \alpha_{k_t} = a_0 \alpha_{k_0} + \cdots + a_{t-1} \alpha_{k_{t-1}} \\
    \sigma(\alpha_{k_t}) = a_0 \sigma(\alpha_{k_0}) + \cdots + a_{t-1} \sigma(\alpha_{k_{t-1}}) \\
    \qquad \qquad \qquad \vdots \\
    \sigma^t(\alpha_{k_t}) = a_0 \sigma^t(\alpha_{k_0}) + \cdots + a_{t-1} \sigma^t(\alpha_{k_{t-1}}).
    \end{cases}
\end{equation}

Para cualquier $j = 0,\dots,t-1$, restamos en \eqref{eq:sistemaec} la ecuación $j + 1$ transformada por $\sigma^{-1}$ de la ecuación $j$. Esto nos lleva al sistema lineal homogéneo

\begin{equation}\label{eq:sistema2}
    \begin{cases}
    0 = \left(a_0 - \sigma^{-1}(a_0)\right)\alpha_{k_0} \\
    \qquad + \cdots + \left(a_{t-1} - \sigma^{-1}(a_{t-1})\right)\alpha_{k_{t-1}} \\
    0 = \left(a_0 - \sigma^{-1}(a_0)\right)\sigma(\alpha_{k_0}) \\
    \qquad + \cdots + \left(a_{t-1} - \sigma^{-1}(a_{t-1})\right)\sigma(\alpha_{k_{t-1}}) \\
    \qquad \qquad \qquad \vdots \\    0 = \left(a_0 - \sigma^{-1}(a_0)\right)\sigma^{t-1}(\alpha_{k_0}) \\
    \qquad + \cdots + \left(a_{t-1} - \sigma^{-1}(a_{t-1})\right)\sigma^{t-1}(\alpha_{k_{t-1}}).
    \end{cases}
    \end{equation}

Por hipótesis de inducción, las primeras $t$ columnas de $\Delta$ son linealmente independientes y, por tanto, $(a_j - \sigma^{-1}(a_j)) = 0$, para todo $j \in \{0,\dots,t-1\}.$ De esta forma, $a_0,\dots,a_{t-1} \in K$, ya que $a_j = \sigma(a_j)$. En consecuencia, la primera ecuación de \ref{eq:sistemaec} nos da dependencia lineal sobre $K$ de la $K$-base $\{\alpha_0,\dots,\alpha_{n-1}\}$, lo que es una contradicción, ya que al ser una base es linealmente independiente. Por tanto $|\Delta| \neq 0$.
\end{proof}

Utilizando la Proposición \ref{prop:norma}(i), el resto de la división por la izquierda de un polinomio $g = \sum_{i=0}^{r}g_ix^i$ por $x - \gamma$ es $\sum_{i=0}^{r}g_iN_i(\gamma)$, donde $N_i$ denota la $i$-ésima norma de $\gamma \in \F(t)$. Siempre que $x - \gamma$ divida por la derecha a $g$, diremos que $\gamma$ es una \emph{raíz por la derecha de $g$}. Recordemos que la noción de $j$-norma también admite una versión para índices negativos, dada por 

$$N_{-j}(\gamma) = \gamma\sigma^{-1}(\gamma)\cdots\sigma^{-j+1}(\gamma).$$

Así, utilizando la Proposición \ref{prop:norma}(ii) el resto de la división por la derecha de un polinomio $g = \sum_{i=0}^{r}g_ix^i$ por $x - \gamma$ puede escribirse como $\sum_{i=0}^{r}\sigma^{-i}(g_i)N^{-i}(\gamma)$. Si $x - \gamma$ divide por la izquierda a $g$ diremos que $\gamma$ es una \emph{raíz por la izquierda de $g$}.

El siguiente lema nos permitirá definir los códigos convolucionales sesgados RS.


\begin{lema}\label{lema:2}
Sea $\alpha$ un elemento del cuerpo de fracciones $\F(t)$ tal que $\{\alpha,\sigma(\alpha),\dots,\sigma^{n-1}(\alpha)\}$ es una base de $\F(t)$ como un $\F(t)^\sigma$-espacio vectorial. Sea $\beta = \alpha^{-1}\sigma(\alpha).$ Para cualquier subconjunto $T = \{t_1 < t_2  < \cdots < t_m\} \subseteq \{0,1,\dots,n-1\}$, los polinomios

\begin{equation}
g^{\ell} = \left[x - \sigma^{t_1}(\beta),x - \sigma^{t_2}(\beta),\dots,x-\sigma^{t_m}(\beta)\right]_\ell
\end{equation}
y
\begin{equation}
    g^{r} = \left[x - \sigma^{t_1}(\beta^{-1}),x - \sigma^{t_2}(\beta^{-1}),\dots,x-\sigma^{t_m}(\beta^{-1})\right]_r
\end{equation}
tienen grado $m$. Por tanto, si $x - \sigma^s(\beta) \ |_r \ g^\ell$ o $x - \sigma^s(\beta^{-1}) \ |_\ell \ g^r$, entonces $s \in T$.
\end{lema}

\begin{proof}
Supongamos que $\gr(g^\ell) < m$, por tanto, $g^\ell = \sum_{i=0}^{m-1}g_ix^i$. Como $g$ es un múltiplo por la izquierda de $x - \sigma^{t_j}(\beta)$ para cualquier $j \in \{1,\dots,m\}$, se tiene por la Proposición \ref{prop:norma}(i) que 
\begin{equation}\label{eq:sishom}
\sum_{i=0}^{m-1}g_iN_i(\sigma^{t_j}(\beta)) = 0
\end{equation}
para cualquier  $j \in \{1,\dots,m\}$.

Esto es un sistema lineal homogéneo cuya matriz de coeficiente es la transpuesta de la matriz

\[ M = 
\begin{bmatrix}
N_0(\sigma^{t_1}(\beta)) & N_0(\sigma^{t_2}(\beta)) & \cdots & N_0(\sigma^{t_m}(\beta)) \\
N_1(\sigma^{t_1}(\beta)) & N_1(\sigma^{t_2}(\beta)) & \cdots & N_1(\sigma^{t_m}(\beta)) \\
N_2(\sigma^{t_1}(\beta)) & N_2(\sigma^{t_2}(\beta)) & \cdots & N_2(\sigma^{t_m}(\beta)) \\
\vdots & \vdots & \ddots & \vdots \\
N_{m-1}(\sigma^{t_1}(\beta)) & N_{m-1}(\sigma^{t_2}(\beta)) & \cdots & N_{m-1}(\sigma^{t_m}(\beta))
\end{bmatrix}.
\]

Observemos que $N_i(\sigma^{t_j}(\beta)) = \sigma^{t_j}(N_i(\beta)) = \sigma^{t_j}(\alpha^{-1})\sigma^{t_j+i}(\alpha)$ para todo  $j \in \{1,\dots,m\}$ y $i \in \{0,\dots,m-1\}$. Por tanto, $|M| = 0$ si y solo si el determinante de la matriz $M'$,

\[
\begin{bmatrix}
\sigma^{t_1}(\alpha) & \sigma^{t_2}(\alpha) & \cdots & \sigma^{t_m}(\alpha) \\
\sigma^{t_1+1}(\alpha) & \sigma^{t_2+1}(\alpha) & \cdots & \sigma^{t_m+1}(\alpha) \\
\sigma^{t_1+2}(\alpha) & \sigma^{t_2+2}(\alpha) & \cdots & \sigma^{t_m+2}(\alpha) \\
\vdots & \vdots & \ddots & \vdots \\
\sigma^{t_1+m-1}(\alpha) & \sigma^{t_2+m-1}(\alpha) & \cdots & \sigma^{t_m+m-1}(\alpha)
\end{bmatrix}
\]
o, equivalentemente,

\[
\begin{bmatrix}
\sigma^{t_1}(\alpha) & \sigma^{t_2}(\alpha) & \cdots & \sigma^{t_m}(\alpha) \\
\sigma(\sigma^{t_1}(\alpha)) & \sigma(\sigma^{t_2}(\alpha)) & \cdots & \sigma(\sigma^{t_m}(\alpha)) \\
\sigma^2(\sigma^{t_1}(\alpha)) & \sigma^2(\sigma^{t_2}(\alpha)) & \cdots & \sigma^2(\sigma^{t_m}(\alpha)) \\
\vdots & \vdots & \ddots & \vdots \\
\sigma^{m-1}(\sigma^{t_1}(\alpha)) & \sigma^{m-1}(\sigma^{t_2}(\alpha)) & \cdots & \sigma^{m-1}(\sigma^{t_m}(\alpha))
\end{bmatrix}
\]

es cero. No obstante, el Lema \ref{lema:sug} nos asegura que $|M'| \neq 0$, por tanto, la única solución del sistema lineal \eqref{eq:sishom} es $g_0 = g_1 = \cdots = g_{m-1} = 0$, lo que es una contradicción. Por tanto, $\gr(g^\ell) = m$. Para el otro polinomio, procedemos de forma similar, si suponemos que $\gr(g^r) < m$ y $g^r = \sum_{i=0}^{m-1}g_ix^i$ obtenemos el sistema lineal 

\begin{equation}
\sum_{i=0}^{m-1}\sigma^{-i}(g_i)N_{-i}(\sigma^{t_j}(\beta^{-1})) = 0
\end{equation}
para cualquier $j \in \{1,\dots,m\}.$
\end{proof}

Observamos ahora que $N_{-i}(\sigma^{t_j}(\beta^{-1})) = \sigma^{t_j}(\alpha^{-1})\sigma^{t_j-i+1}(\alpha)$ para $i \in \{0,\dots,m-1\}$ y $j \in \{1,\dots,m\}$. Entonces, por el Lema \ref{lema:sug}, el sistema tiene una única solución $\sigma^{-i}(g_i) = 0$ para $\{0,\dots,m-1\}$, así, $g_0 = \dots = g_{m-1} = 0$. De nuevo, esto es una contradicción y $\gr(g^r) = m$.

\begin{definicion}\label{def:RS}
Sean $\alpha,\beta \in \F(t)$ verificando las condiciones del Lema \ref{lema:2}. Un código convolucional sesgado Reed-Solomon (RS) de distancia designada $\delta \leq n$ es un SCCC generado por 

$$[x - \sigma^r(\beta),x-\sigma^{r+1}(\beta),\dots,x - \sigma^{r + \delta - 2}(\beta)]_\ell,$$
para algún $r \geq 0.$
\end{definicion}

El siguiente teorema nos asegura que los códigos convolucionales sesgados RS son MDS con respecto a la distancia de Hamming.

\begin{teorema}\label{th:dist}
Sea $\C$ un código convolucional sesgado RS de distancia designada $\delta$. La distancia de Hamming de $\C$ es $\delta$.
\end{teorema}

\begin{proof}
Sea $$g = [x - \sigma^r(\beta), x - \sigma^{r+1}(\beta),\dots,x - \sigma^{r + \delta - 2}(\beta)]_\ell,$$
un generador de $\C$ como un ideal izquierdo de $\R_n$. Una matriz de paridad de $\C$ es

\[ H = 
\begin{bmatrix}
N_0(\sigma^r(\beta)) & N_0(\sigma^{r+1}(\beta)) & \cdots & N_0(\sigma^{r+\delta-2}(\beta)) \\
N_1(\sigma^r(\beta)) & N_1(\sigma^{r+1}(\beta)) & \cdots & N_1(\sigma^{r+\delta-2}(\beta)) \\
N_2(\sigma^r(\beta)) & N_2(\sigma^{r+1}(\beta)) & \cdots & N_2(\sigma^{r+\delta-2}(\beta)) \\
\vdots & \vdots & \ddots & \vdots \\
N_{n-1}(\sigma^r(\beta)) & N_{n-1}(\sigma^{r+1}(\beta)) & \cdots & N_{n-1}(\sigma^{r+\delta-2}(\beta))
\end{bmatrix},
\]

ya que sus columnas nos dan las evaluaciones por la derecha de las raíces. Debemos probar que cualquier $(\delta - 1)$-menor de $H$ no es nulo. Vamos a proceder de manera parecida a como lo hicimos en el Lema \ref{lema:2}. Observemos que $N_i(\sigma^k(\beta)) = \sigma^k(N_i(\beta)) = \sigma^k(\alpha^{-1})\sigma^{i+k}(\alpha)$ para cualesquiera enteros $i$ y $k$. Por tanto, dada una submatriz $M$ de dimensión $(\delta - 1)\times(\delta - 1),$

\[
\begin{bmatrix}
N_{k_1}(\sigma^r(\beta)) & N_{k_1}(\sigma^{r+1}(\beta)) & \cdots & N_{k_1}(\sigma^{r+\delta-2}(\beta)) \\
N_{k_2}(\sigma^r(\beta)) & N_{k_2}(\sigma^{r+1}(\beta)) & \cdots & N_{k_2}(\sigma^{r+\delta-2}(\beta)) \\
N_{k_3}(\sigma^r(\beta)) & N_{k_3}(\sigma^{r+1}(\beta)) & \cdots & N_{k_3}(\sigma^{r+\delta-2}(\beta)) \\
\vdots & \vdots & \ddots & \vdots \\
N_{k_{\delta-1}}(\sigma^r(\beta)) & N_{k_{\delta-1}}(\sigma^{r+1}(\beta)) & \cdots & N_{k_{\delta-1}}(\sigma^{r+\delta-2}(\beta))
\end{bmatrix},
\]

con $\{k_1 < k_2 < \cdots < k_{\delta-1}\} \subset \{0,1,\dots,n-1\},$ $|M| = 0$ si y solo si $|M'| = 0$, donde $M'$ es la matriz

\[
\begin{bmatrix}
\sigma^{k_1 + r}(\alpha) & \sigma^{k_1 + r + 1}(\alpha) & \cdots & \sigma^{k_1 + r + \delta - 2}(\alpha) \\
\sigma^{k_2 + r}(\alpha) & \sigma^{k_2 + r + 1}(\alpha) & \cdots & \sigma^{k_2 + r + \delta - 2}(\alpha) \\
\sigma^{k_3 + r}(\alpha) & \sigma^{k_3 + r + 1}(\alpha) & \cdots & \sigma^{k_3 + r + \delta - 2}(\alpha) \\
\vdots & \vdots & \ddots & \vdots \\
\sigma^{k_{\delta - 1} + r}(\alpha) & \sigma^{k_{\delta - 1} + r + 1}(\alpha) & \cdots & \sigma^{k_{\delta - 1} + r + \delta - 2}(\alpha) \\
\end{bmatrix} =
\] 

\[ = 
\begin{bmatrix}
\sigma^{k_1 + r}(\alpha) & \sigma(\sigma^{k_1 + r}(\alpha)) & \cdots & \sigma^{\delta - 2}(\sigma^{k_1 + r}(\alpha)) \\
\sigma^{k_2 + r}(\alpha) & \sigma(\sigma^{k_2 + r}(\alpha)) & \cdots & \sigma^{\delta - 2}(\sigma^{k_2 + r}(\alpha)) \\
\sigma^{k_3 + r}(\alpha) & \sigma(\sigma^{k_3 + r}(\alpha)) & \cdots & \sigma^{\delta - 2}(\sigma^{k_3 + r}(\alpha)) \\
\vdots & \vdots & \ddots & \vdots \\
\sigma^{k_{\delta - 1} + r}(\alpha) & \sigma(\sigma^{k_{\delta - 1} + r}(\alpha)) & \cdots & \sigma^{\delta - 2}(\sigma^{k_{\delta - 1} + r}(\alpha)) \\
\end{bmatrix} 
\] 

Como $\{\alpha,\sigma(\alpha),\dots,\sigma^{n-1}(\alpha)\}$ es una base de la extensión de cuerpos $\F(t)^\sigma \subset \F(t)$, por el Lema \ref{lema:sug}, $|M'| \neq 0$.
\end{proof}

Cada función racional $f(t) \in \F(t)$ puede ser vista como una serie de potencias formal de Laurent. Así, cada palabra código $v(t) \in \F(t)^n$ se puede expresar mediante una serie de Laurent con coeficientes en $\F^n$. De esta forma,

$$v(t) = \sum_{i=m}^{+\infty}v_it^i, v_i \in \F^n, m \in \Z.$$

Por tanto, podemos definir el \emph{peso} de $v(t) \in \C$ como la suma de los pesos de Hamming de todos sus coeficientes. Dado un código convolucional, es decir, un $\F(t)$-subespacio vectorial $\C$ de $\F(t)^n$, su distancia libre se define como el mínimo de los pesos de todas las palabras código $v(t) \in \C$. Es evidente que wt($v(t)$) será finito si y solo si $v(t)$ es un polinomio de Laurent.

Esta discusión es un poco distinta a la que hicimos en la sección \ref{sec:distancias}. Esto se debe a que usamos la definición de código convolucional como sumando directo del módulo libre $\F[t]^n$. Sin embargo, dado $k \in \{0,\dots,n\}$ la Proposición \ref{prop:relacion} nos dice que la aplicación $\C \mapsto \C \cap \F[t]^n$ establece una correspondencia biyectiva entre el conjunto de todos los $\F(t)$-subespacios vectoriales $k$-dimensionales de $\F(t)^n$ y el conjunto de todos los $\F[t]$-submódulos de rango $k$ de $\F[t]^n$ que son sumandos directos. Por tanto,

$$d_{\text{free}}(\C) = \min\{\text{wt}(v(t)) : 0 \neq v(t) \in \C \cap \F[t]^n\}.$$ 

De esta forma, la distancia libre de $\C$ es igual a la distancia libre de $\C \cap \F[t]^n$ tal y como la definimos en \ref{def:libre}.

El siguiente resultado nos da una cota para la distancia libre de un código convolucional sesgado RS.

\begin{corolario}
Sea $\C$ un código convolucional sesgado RS de longitud $n$ y dimensión $k$. Entonces

\begin{equation}
n - k + 1 \leq d_{\text{free}}(\C) \leq (n-k)(\lfloor m/k \rfloor + 1) + m + 1,
\end{equation}

donde $m$ es el grado interno de $\C \cap \F[t]^n$.

\end{corolario}

\begin{proof}
Un código convolucional sesgado RS de longitud $n$ y dimensión $k$ tiene distancia designada de Hamming $\delta = n - k + 1$. Por el Teorema \ref{th:dist}, la distancia de Hamming de $\C$ es $n - k +1$. La distancia libre de $\C$ es siempre más grande que su distancia de Hamming y menor que la cota generalizada de Singleton por el Teorema \ref{th:singleton2}. 
    
\end{proof}






\endinput